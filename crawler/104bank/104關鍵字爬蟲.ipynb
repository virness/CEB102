{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config]\n",
      "\tKeyword:\t\t\t雲端服務\n",
      "\tPages:\t\t\t\tALL\n",
      "\tSave separately:\t\t0\n",
      "\tCache:\t\t\t\t15\n",
      "\n",
      "\n",
      "[Querying for 雲端服務 ...]\n",
      "Computing the amount of title... (1710)\n",
      "\n",
      "\n",
      "[Loading data...]\n",
      "Page 1 ...\t================================================================Done!\n",
      "Page 2 ...\t==============================================================Done!\n",
      "Page 3 ...\t==============================================================Done!\n",
      "Page 4 ...\t==============================================================Done!\n",
      "Page 5 ...\t==============================================================Done!\n",
      "Page 6 ...\t==============================================================Done!\n",
      "Page 7 ...\t==============================================================Done!\n",
      "Page 8 ...\t==============================================================Done!\n",
      "Page 9 ...\t==============================================================Done!\n",
      "Page 10 ...\t==============================================================Done!\n",
      "Page 11 ...\t==============================================================Done!\n",
      "Page 12 ...\t==============================================================Done!\n",
      "Page 13 ...\t==============================================================Done!\n",
      "Page 14 ...\t==============================================================Done!\n",
      "Page 15 ...\t==============================================================Done!\n",
      "----------\n",
      "Take a break for 30 sec.\n",
      "----------\n",
      "Page 16 ...\t==============================================================Done!\n",
      "Page 17 ...\t==============================================================Done!\n",
      "Page 18 ...\t==============================================================Done!\n",
      "Page 19 ...\t==============================================================Done!\n",
      "Page 20 ...\t==============================================================Done!\n",
      "Page 21 ...\t==============================================================Done!\n",
      "Page 22 ...\t==============================================================Done!\n",
      "Page 23 ...\t==============================================================Done!\n",
      "Page 24 ...\t==============================================================Done!\n",
      "Page 25 ...\t==============================================================Done!\n",
      "Page 26 ...\t==============================================================Done!\n",
      "Page 27 ...\t==============================================================Done!\n",
      "Page 28 ...\t==============================================================Done!\n",
      "Page 29 ...\t==============================================================Done!\n",
      "Page 30 ...\t==============================================================Done!\n",
      "----------\n",
      "Take a break for 30 sec.\n",
      "----------\n",
      "Page 31 ...\t==============================================================Done!\n",
      "Page 32 ...\t==============================================================Done!\n",
      "Page 33 ...\t==============================================================Done!\n",
      "Page 34 ...\t==============================================================Done!\n",
      "Page 35 ...\t==============================================================Done!\n",
      "Page 36 ...\t==============================================================Done!\n",
      "Page 37 ...\t==============================================================Done!\n",
      "Page 38 ...\t==============================================================Done!\n",
      "Page 39 ...\t==============================================================Done!\n",
      "Page 40 ...\t==============================================================Done!\n",
      "Page 41 ...\t==============================================================Done!\n",
      "Page 42 ...\t==============================================================Done!\n",
      "Page 43 ...\t==============================================================Done!\n",
      "Page 44 ...\t==============================================================Done!\n",
      "Page 45 ...\t==============================================================Done!\n",
      "----------\n",
      "Take a break for 30 sec.\n",
      "----------\n",
      "Page 46 ...\t==============================================================Done!\n",
      "Page 47 ...\t==============================================================Done!\n",
      "Page 48 ...\t==============================================================Done!\n",
      "Page 49 ...\t==============================================================Done!\n",
      "Page 50 ...\t==============================================================Done!\n",
      "Page 51 ...\t==============================================================Done!\n",
      "Page 52 ...\t==============================================================Done!\n",
      "Page 53 ...\t==============================================================Done!\n",
      "Page 54 ...\t==============================================================Done!\n",
      "Page 55 ...\t==============================================================Done!\n",
      "Page 56 ...\t==============================================================Done!\n",
      "Page 57 ...\t==============================================================Done!\n",
      "Page 58 ...\t==============================================================Done!\n",
      "Page 59 ...\t==============================================================Done!\n",
      "Page 60 ...\t==============================================================Done!\n",
      "----------\n",
      "Take a break for 30 sec.\n",
      "----------\n",
      "Page 61 ...\t==============================================================Done!\n",
      "Page 62 ...\t==============================================================Done!\n",
      "Page 63 ...\t==============================================================Done!\n",
      "Page 64 ...\t==============================================================Done!\n",
      "Page 65 ...\t==============================================================Done!\n",
      "Page 66 ...\t==============================================================Done!\n",
      "Page 67 ...\t==============================================================Done!\n",
      "Page 68 ...\t==============================================================Done!\n",
      "Page 69 ...\t==============================================================Done!\n",
      "Page 70 ...\t==============================================================Done!\n",
      "Page 71 ...\t==============================================================Done!\n",
      "Page 72 ...\t==============================================================Done!\n",
      "Page 73 ...\t==============================================================Done!\n",
      "Page 74 ...\t==============================================================Done!\n",
      "Page 75 ...\t==============================================================Done!\n",
      "----------\n",
      "Take a break for 30 sec.\n",
      "----------\n",
      "Page 76 ...\t==============================================================Done!\n",
      "Page 77 ...\t==============================================================Done!\n",
      "Page 78 ...\t==============================================================Done!\n",
      "Page 79 ...\t==============================================================Done!\n",
      "Page 80 ...\t==============================================================Done!\n",
      "Page 81 ...\t==============================================================Done!\n",
      "Page 82 ...\t==============================================================Done!\n",
      "Page 83 ...\t==============================================================Done!\n",
      "Page 84 ...\t==============================================================Done!\n",
      "Page 85 ...\t==============================================================Done!\n",
      "Page 86 ...\t====                                                          Empty!\n",
      "File has saved to ./job104_resource/雲端服務_2021-03-11_0546/title_url.xlsx\n",
      "\n",
      "[Done!]\n",
      "\n",
      "\n",
      "[Computing the amount of each skill...]\n",
      "Processes all done.\n",
      "\n",
      "Check the following directories.\n",
      "./job104_resource/雲端服務_2021-03-11_0546\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "# import pprint\n",
    "import pandas as pd\n",
    "import re\n",
    "import threading\n",
    "import random\n",
    "import xlsxwriter\n",
    "\n",
    "# Load dictionary\n",
    "path = './dict'\n",
    "dictionary_list = os.listdir(path)\n",
    "word_list = list()\n",
    "for f in dictionary_list:\n",
    "    with open(r'%s/%s'%(path, f), 'r', encoding='utf-8') as d:\n",
    "        word_list += d.read().split('\\n')\n",
    "\n",
    "# Header\n",
    "headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "   'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "   'Accept-Encoding': 'none',\n",
    "   'Accept-Language': 'en-US,en;q=0.8',\n",
    "   'Connection': 'keep-alive'}\n",
    "\n",
    "# Create a directory to saving files\n",
    "path = r'./job104_resource'\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "work_path = r'./work_dir'\n",
    "if not os.path.isdir(work_path):\n",
    "    os.mkdir(work_path)\n",
    "\n",
    "# synonym dictionary\n",
    "synonym_dict = {}\n",
    "synonym_path = r'./synonym/synonym.txt'\n",
    "with open(synonym_path, 'r', encoding='utf-8') as syn:\n",
    "    syn_str = syn.read().split('\\n')\n",
    "for each_row in syn_str:\n",
    "    synonym_dict[each_row.split(',')[0]] = [item for item in each_row.split(',')]\n",
    "\n",
    "# Jieba then replace by synonym dictionary\n",
    "def dealWithSynonym(long_str):\n",
    "    # Select words we need according to /dict\n",
    "    # and append each word to tmp_list\n",
    "    tmp_list = []\n",
    "    long_str.replace(' ','')\n",
    "    for word_select in word_list:\n",
    "        if word_select in long_str:\n",
    "            if word_select.upper() == 'JAVA' or word_select.upper() == 'JAVASCRIPT':\n",
    "                continue\n",
    "            elif word_select == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0] != 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) != None:\n",
    "                # print(1,word_select)\n",
    "                long_str = long_str.replace(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0], '')\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                if word_select in long_str:\n",
    "                    tmp_list.append(word_select.upper())\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                continue\n",
    "            elif word_select == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == None:\n",
    "                # print(2,word_select)\n",
    "                continue\n",
    "            else:\n",
    "                # print(3,word_select)\n",
    "                tmp_list.append(word_select.upper())\n",
    "    long_str = long_str.upper()\n",
    "    for word_select in word_list:\n",
    "        if (word_select.upper() in long_str) and (not word_select.upper() in tmp_list):\n",
    "            if word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0] != 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) != None:\n",
    "                # print(1, word_select)\n",
    "                long_str = long_str.replace(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0], '')\n",
    "                if re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == 'R':\n",
    "                    tmp_list.append(word_select.upper())\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                continue\n",
    "            elif word_select.upper() == 'JAVA':\n",
    "                continue\n",
    "            elif word_select.upper() == 'JAVASCRIPT':\n",
    "                long_str = long_str.replace('JAVASCRIPT', '')\n",
    "                tmp_list.append(word_select.upper())\n",
    "            elif word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == None:\n",
    "                # print(2, word_select)\n",
    "                continue\n",
    "            else:\n",
    "                # print(3, word_select)\n",
    "                tmp_list.append(word_select.upper())\n",
    "    long_str = long_str.replace('JAVASCRIPT', '')\n",
    "    for word_select in word_list:\n",
    "        if (word_select.upper() in long_str) and (not word_select.upper() in tmp_list):\n",
    "            if word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0] != 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) != None:\n",
    "                # print(1, word_select)\n",
    "                long_str = long_str.replace(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0], '')\n",
    "                if re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == 'R':\n",
    "                    tmp_list.append(word_select.upper())\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                continue\n",
    "            elif word_select.upper() == 'JAVA':\n",
    "                tmp_list.append(word_select.upper())\n",
    "            elif word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == None:\n",
    "                # print(2, word_select)\n",
    "                continue\n",
    "            else:\n",
    "                # print(3, word_select)\n",
    "                tmp_list.append(word_select.upper())\n",
    "\n",
    "    # Replace Synonym\n",
    "    for word_key in synonym_dict:\n",
    "        for word_value in  synonym_dict[word_key]:\n",
    "            for num, operating_word in enumerate(tmp_list):\n",
    "                if operating_word.upper() == word_value.upper():\n",
    "                    tmp_list[num] = word_key\n",
    "    tmp_list = list(set(tmp_list))\n",
    "\n",
    "    tmp_str = ''\n",
    "    for n, w in enumerate(tmp_list):\n",
    "        tmp_str += w\n",
    "        if n < len(tmp_list) - 1:\n",
    "            tmp_str += ','\n",
    "\n",
    "    return tmp_str\n",
    "\n",
    "def getSkill(titleUrl):\n",
    "    ## modified 20200306\n",
    "    tmp_title_count = 0\n",
    "    with open(r'./work_dir/.title_count.txt', 'r', encoding='utf-8') as f:\n",
    "        tmp_title_count = int(f.read().split('\\n')[0])\n",
    "    with open(r'./work_dir/.title_count.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(str(tmp_title_count + 1))\n",
    "    ##\n",
    "    job_content = '工作內容'\n",
    "    job_require = '條件要求'\n",
    "    job_welfare = '公司福利'\n",
    "    job_contact = '聯絡方式'\n",
    "    tmp_list = [job_content, job_require, job_welfare, job_contact]\n",
    "\n",
    "    job_skill_data = ''\n",
    "\n",
    "    try:\n",
    "        res = requests.get(titleUrl, headers=headers)\n",
    "    except:\n",
    "        print('Error', 'getSkill(titleUrl)', titleUrl)\n",
    "        print('Wait a moment!')\n",
    "        time.sleep(4)\n",
    "        return tmp_list[0], tmp_list[1], tmp_list[2], tmp_list[3], job_skill_data\n",
    "\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    ## modified 20200305\n",
    "    #contents = soup.select('div.content')\n",
    "    \n",
    "    # modified 20200305\n",
    "    # job_content\n",
    "    try:\n",
    "        job_content = soup.select('div.content')[1].text\n",
    "    except:\n",
    "        job_content = '工作內容'\n",
    "    \n",
    "    # job_require\n",
    "    try:\n",
    "        job_require= soup.select('div.content')[3].text +\\\n",
    "                     soup.select('div.content')[4].text\n",
    "    except:\n",
    "        job_require = '條件要求'\n",
    "    \n",
    "    #job_welfare\n",
    "    try:\n",
    "        job_welfare = soup.select('div.content')[2].text\n",
    "    except:\n",
    "        job_welfare = '公司福利'\n",
    "    \n",
    "    # job_contact\n",
    "    try:\n",
    "        job_contact = soup.select('div.content')[5].table.text\n",
    "    except:\n",
    "        job_contact = '聯絡方式'\n",
    "    \n",
    "    tmp_list = [job_content, job_require, job_welfare, job_contact]\n",
    "    ##\n",
    "\n",
    "    return tmp_list[0], tmp_list[1], tmp_list[2], tmp_list[3], dealWithSynonym(re.sub(r'[-:_0-9、【】：)(，.&+]', '', (tmp_list[0].replace('\\n', '').replace('\\t', '') + tmp_list[1].replace('\\n', '').replace('\\t', ''))))\n",
    "\n",
    "'''\n",
    "Keyword for a list of title and url\n",
    "[[\"Title1\", \"URL1\", \"Skill\"], [\"Title2\", \"URL2\", \"Skill\"]]\n",
    "'''\n",
    "timenow = time.strftime(\"%Y-%m-%d_%H%M\")\n",
    "def keywordForTitle(keyword, max_page = 0, save_separately = 0, cache = 15, from_page = 1):\n",
    "\n",
    "    # Create a directory\n",
    "    path = r'./job104_resource/%s_%s'%(keyword, timenow)\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    work_path = r'./work_dir/%s'%(keyword)\n",
    "    if not os.path.isdir(work_path):\n",
    "        os.mkdir(work_path)\n",
    "\n",
    "    col_path = r'./config/col.txt'\n",
    "    ohencoding_col = open(col_path, 'r', encoding='utf-8').read().lower().split('\\n')\n",
    "\n",
    "    col = ['Job_company', 'Job Openings','Job_content', 'Job_require', 'Job_welfare', 'Job_contact', 'URL']\n",
    "    if len(ohencoding_col) > 0:\n",
    "        col += ohencoding_col\n",
    "    df = pd.DataFrame(columns=col)\n",
    "\n",
    "    pages = from_page\n",
    "    title_url_list = list()\n",
    "    # skill data\n",
    "    job_skill_data_sum = ''\n",
    "    while True:\n",
    "        process_tag = 1\n",
    "\n",
    "        print('Page %s ...\\t==' % (pages), end='')\n",
    "        url = 'https://www.104.com.tw/jobs/search/?ro=0&keyword=%s&order=1&asc=0&page=%s&mode=s&jobsource=2018indexpoc'%(keyword, pages)\n",
    "        try:\n",
    "            res = requests.get(url, headers=headers)\n",
    "        except:\n",
    "            print('Error', 'keywordForTitle(keyword, max_page = 0)', url)\n",
    "            print('Wait a moment!')\n",
    "            time.sleep(4)\n",
    "            continue\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        title = soup.select('div[class=\"b-block__left\"] h2[class=\"b-tit\"] a')\n",
    "        company = soup.select('div[class=\"b-block__left\"] ul[class=\"b-list-inline b-clearfix\"] li a')\n",
    "        print('==', end='')\n",
    "\n",
    "        # Stoping conditions\n",
    "        # If the page is empty -> stop\n",
    "        if len(title) == 0:\n",
    "            if process_tag < 30:\n",
    "                print('  ' * (30 - process_tag), end='')\n",
    "            print('Empty!')\n",
    "            break\n",
    "\n",
    "        # [[\"Title1\", \"URL1\"], [\"Title2\", \"URL2\"]]\n",
    "        for num, each_title in enumerate(title):\n",
    "            ohencoding_col_tmp = []\n",
    "            if len(ohencoding_col) > 0:\n",
    "                ohencoding_col_tmp = [0 for item in ohencoding_col]\n",
    "            tmp_title = each_title.text\n",
    "            tmp_url = each_title['href'].replace('//', 'https://')\n",
    "            ## modified 2020-03-05\n",
    "            tmp_url = tmp_url.replace('www', 'm')\n",
    "            ##\n",
    "            \n",
    "            # job_skill_data is a list of all skills\n",
    "            job_content, job_require, job_welfare, job_contact, job_skill_data = getSkill(tmp_url)\n",
    "            # skill data\n",
    "            job_skill_data_sum += (job_skill_data + ',')\n",
    "\n",
    "            # Do one-hot encoding\n",
    "            title_url_list.append([tmp_title, tmp_url])\n",
    "            for index, skill_col in enumerate(ohencoding_col):\n",
    "                for check_skill in job_skill_data.split(','):\n",
    "                    if check_skill.upper() == skill_col.upper():\n",
    "                        ohencoding_col_tmp[index] = 1\n",
    "                        continue\n",
    "            tmp_col = [company[num].text, tmp_title, job_content, job_require, job_welfare, job_contact, tmp_url] + ohencoding_col_tmp\n",
    "            df = df.append(pd.DataFrame([tmp_col], columns=col), ignore_index=True)\n",
    "            print('==', end='')\n",
    "            process_tag += 1\n",
    "            time.sleep(random.randint(3,8)/10)\n",
    "\n",
    "         # save skill data to a file every 15 pages\n",
    "        if pages % cache == 0:\n",
    "            with open(r'%s/%s'%(work_path, pages), 'w', encoding='utf-8') as skill:\n",
    "                skill.write(job_skill_data_sum)\n",
    "            job_skill_data_sum = ''\n",
    "\n",
    "            if save_separately != 0:\n",
    "                df.to_excel(r'%s/title_url_%s.xlsx' % (path, time.strftime(\"%Y-%m-%d_%H%M\")), engine='xlsxwriter')\n",
    "                col = ['Job_company', 'Job Openings', 'Job_content', 'Job_require', 'Job_welfare', 'Job_contact', 'URL']\n",
    "                if len(ohencoding_col) > 0:\n",
    "                    col += ohencoding_col\n",
    "                df = pd.DataFrame(columns=col)\n",
    "\n",
    "        # Stoping conditions\n",
    "        if max_page != 0:\n",
    "            if pages >= max_page:\n",
    "                if process_tag < 30:\n",
    "                    print('==' * (30 - process_tag), end='')\n",
    "                print('Done!')\n",
    "                break\n",
    "\n",
    "        if process_tag < 30:\n",
    "            print('=='*(30-process_tag), end='')\n",
    "        print('Done!')\n",
    "        if pages % cache == 0 and save_separately != 0:\n",
    "            print('File has saved to %s' % (path))\n",
    "\n",
    "        # Pause for 30 sec every 15 pages\n",
    "        if pages % 15 == 0:\n",
    "            print('----------\\nTake a break for 30 sec.\\n----------')\n",
    "            time.sleep(30)\n",
    "        pages += 1\n",
    "\n",
    "    with open(r'%s/%s' % (work_path, pages), 'w', encoding='utf-8') as skill:\n",
    "        skill.write(job_skill_data_sum)\n",
    "\n",
    "    df.to_excel(r'%s/title_url_%s.xlsx'%(path, time.strftime(\"%Y-%m-%d_%H%M\")), engine='xlsxwriter')\n",
    "    print('File has saved to %s/title_url.xlsx'%(path))\n",
    "    return title_url_list\n",
    "\n",
    "# Count title amount\n",
    "def keywordForTitle_countTitle(keyword, max_page = 0, from_page = 1):\n",
    "\n",
    "    pages = from_page\n",
    "    count_title = 0\n",
    "    while True:\n",
    "        url = 'https://www.104.com.tw/jobs/search/?ro=0&keyword=%s&order=1&asc=0&page=%s&mode=s&jobsource=2018indexpoc'%(keyword, pages)\n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        title = soup.select('div[class=\"b-block__left\"] h2[class=\"b-tit\"] a')\n",
    "\n",
    "        # Stoping conditions\n",
    "        # If the page is empt -> stop\n",
    "        if len(title) == 0:\n",
    "            break\n",
    "\n",
    "        # [[\"Title1\", \"URL1\"], [\"Title2\", \"URL2\"]]\n",
    "        for each_title in title:\n",
    "            count_title += 1\n",
    "\n",
    "        # Stoping conditions\n",
    "        if max_page != 0:\n",
    "            if pages >= max_page:\n",
    "                break\n",
    "        pages += 1\n",
    "    return count_title\n",
    "\n",
    "# Map reduce\n",
    "def mrThread(file_path, mr_path, save_name):\n",
    "    mr_dict = {}\n",
    "    tmp_str = ''\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tmp_list = f.read().replace('\\n','').split(',')\n",
    "    for w in tmp_list:\n",
    "        if w in mr_dict:\n",
    "            mr_dict[w] += 1\n",
    "        else:\n",
    "            mr_dict[w] = 1\n",
    "\n",
    "    for d in mr_dict:\n",
    "        tmp_str += '%s:%s\\n'%(d, mr_dict[d])\n",
    "\n",
    "    with open(r'%s/%s'%(mr_path, save_name), 'w', encoding='utf-8') as f:\n",
    "        f.write(tmp_str)\n",
    "    # return tmp_str\n",
    "\n",
    "def main():\n",
    "    kyword = '大數據分析'\n",
    "    pages = 0\n",
    "    save_separately = 1\n",
    "    cache = 15\n",
    "    ori_par = [kyword, pages, save_separately, cache]\n",
    "    with open(r'./config/conf.txt', 'r', encoding='utf-8') as con:\n",
    "        par = con.read().split('\\n')\n",
    "    # print(par)\n",
    "    for n, p in enumerate(par):\n",
    "        par[n] = p.split('=')[1].replace(' ', '')\n",
    "        if n != 0:\n",
    "            par[n] = int(par[n])\n",
    "        if n == 1 or n == 2:\n",
    "            if par[n] < 0:\n",
    "                par[n] = ori_par[n]\n",
    "        if n == 3:\n",
    "            if par[n] < 1:\n",
    "                par[n] = ori_par[n]\n",
    "    # print(par)\n",
    "    kyword = par[0]\n",
    "    pages = par[1]\n",
    "    save_separately = par[2]\n",
    "    cache = par[3]\n",
    "\n",
    "    print('[Config]')\n",
    "    print('\\tKeyword:\\t\\t\\t%s'%(kyword))\n",
    "    if pages == 0:\n",
    "        print('\\tPages:\\t\\t\\t\\t%s' % ('ALL'))\n",
    "    else:\n",
    "        print('\\tPages:\\t\\t\\t\\t%s' % (pages))\n",
    "    print('\\tSave separately:\\t\\t%s' % (save_separately))\n",
    "    print('\\tCache:\\t\\t\\t\\t%s' % (cache))\n",
    "    print('\\n')\n",
    "\n",
    "    print('[Querying for %s ...]'%(kyword))\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Compute the amount of title\n",
    "    print('Computing the amount of title... ', end='')\n",
    "    title_amount = keywordForTitle_countTitle(kyword, pages)\n",
    "    print('(%s)'%(title_amount))\n",
    "    print('\\n')\n",
    "    ## modified 20200306\n",
    "    with open(r'./work_dir/.title_amount.txt', 'w') as f:\n",
    "        f.write(str(title_amount))\n",
    "    with open(r'./work_dir/.title_count.txt', 'w') as f:\n",
    "        f.write('0')\n",
    "    ##\n",
    "    time.sleep(2)\n",
    "\n",
    "    print('[Loading data...]')\n",
    "    # Crawl all title, url and content and save as file\n",
    "    keyword_for_title_and_url = keywordForTitle(kyword, pages, save_separately, cache)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Print data\n",
    "    # pprint.pprint(keyword_for_title_and_url)\n",
    "    print('')\n",
    "    print('[Done!]')\n",
    "\n",
    "    # Map-reduce\n",
    "    time.sleep(1)\n",
    "    print('\\n')\n",
    "    print('[Computing the amount of each skill...]')\n",
    "\n",
    "    work_path = r'./work_dir/%s' % (kyword)\n",
    "    mr_path = r'%s/mr_%s' % (work_path, time.strftime(\"%Y-%m-%d_%H%M\"))\n",
    "\n",
    "    # cache_list = os.listdir(work_path)\n",
    "    cache_list = [f for f in os.listdir(work_path) if f[0:2] != 'mr']\n",
    "    threadList = list()\n",
    "    for save_name, thr in enumerate(cache_list):\n",
    "        threadList.append(threading.Thread(target=mrThread, args=(r'%s/%s' % (work_path, thr), mr_path, save_name)))\n",
    "\n",
    "    if not os.path.isdir(mr_path):\n",
    "        os.mkdir(mr_path)\n",
    "\n",
    "    for i in threadList:\n",
    "        i.start()\n",
    "        # time.sleep(0.1)\n",
    "    for i in threadList:\n",
    "        i.join()\n",
    "\n",
    "    tmp_mr = []\n",
    "    tmp_mr_dict = {}\n",
    "    for i in os.listdir(mr_path):\n",
    "        with open(r'%s/%s'%(mr_path, i), 'r', encoding='utf-8') as f:\n",
    "            tmp_mr += f.read().split('\\n')\n",
    "    # print(tmp_mr)\n",
    "    for i in tmp_mr:\n",
    "        if len(i.split(':')) != 2 or i.split(':')[0] == '':\n",
    "            continue\n",
    "        if i.split(':')[0] in tmp_mr_dict:\n",
    "            tmp_mr_dict[i.split(':')[0]] += int(i.split(':')[1])\n",
    "        else:\n",
    "            tmp_mr_dict[i.split(':')[0]] = int(i.split(':')[1])\n",
    "\n",
    "    tmp_str = ''\n",
    "    for d in tmp_mr_dict:\n",
    "        tmp_str += '%s:%s\\n'%(d, tmp_mr_dict[d])\n",
    "\n",
    "    path = r'./job104_resource/%s_%s' % (kyword, timenow)\n",
    "    with open(r'%s/map_reduce_%s.txt'%(path, timenow), 'w', encoding='utf-8') as f:\n",
    "        f.write(tmp_str)\n",
    "\n",
    "    # Remove temporary MR file\n",
    "    for rm in cache_list:\n",
    "        os.remove(r'%s/%s'%(work_path, rm))\n",
    "\n",
    "    print('Processes all done.\\n')\n",
    "    print('Check the following directories.')\n",
    "    print('./job104_resource/%s_%s'%(kyword, timenow))\n",
    "    with open(r'./work_dir/.file_status.txt', 'w') as f:\n",
    "        f.write('job104_resource/%s_%s'%(kyword, timenow))\n",
    "\n",
    "    time.sleep(2)\n",
    "    \n",
    "    outStr = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <link rel=\"stylesheet\" href=\"https://www.w3schools.com/w3css/4/w3.css\">\n",
    "    </head>\n",
    "    <body bgcolor=\"green\">\n",
    "    <div class=\"w3-animate-opacity\" align=\"center\">\n",
    "    ALL DONE!\n",
    "    </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    return outStr\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "# import pprint\n",
    "import pandas as pd\n",
    "import re\n",
    "import threading\n",
    "import random\n",
    "import xlsxwriter\n",
    "\n",
    "# 讀取字典\n",
    "path = './dict'\n",
    "dictionary_list = os.listdir(path)\n",
    "word_list = list()\n",
    "for f in dictionary_list:\n",
    "    with open(r'%s/%s'%(path, f), 'r', encoding='utf-8') as d:\n",
    "        word_list += d.read().split('\\n')\n",
    "\n",
    "# Header\n",
    "headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "   'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "   'Accept-Encoding': 'none',\n",
    "   'Accept-Language': 'en-US,en;q=0.8',\n",
    "   'Connection': 'keep-alive'}\n",
    "\n",
    "path = r'./job104_resource'\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "work_path = r'./work_dir'\n",
    "if not os.path.isdir(work_path):\n",
    "    os.mkdir(work_path)\n",
    "    \n",
    "    \n",
    "synonym_dict = {}\n",
    "synonym_path = r'./synonym/synonym.txt'\n",
    "with open(synonym_path, 'r', encoding='utf-8') as syn:\n",
    "    syn_str = syn.read().split('\\n')\n",
    "for each_row in syn_str:\n",
    "    synonym_dict[each_row.split(',')[0]] = [item for item in each_row.split(',')]\n",
    "    \n",
    "# Jieba then replace by synonym dictionary\n",
    "def dealWithSynonym(long_str):\n",
    "    # Select words we need according to /dict\n",
    "    # and append each word to tmp_list\n",
    "    tmp_list = []\n",
    "    long_str.replace(' ','')\n",
    "    for word_select in word_list:\n",
    "        if word_select in long_str:\n",
    "            if word_select.upper() == 'JAVA' or word_select.upper() == 'JAVASCRIPT':\n",
    "                continue\n",
    "            elif word_select == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0] != 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) != None:\n",
    "                # print(1,word_select)\n",
    "                long_str = long_str.replace(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0], '')\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                if word_select in long_str:\n",
    "                    tmp_list.append(word_select.upper())\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                continue\n",
    "            elif word_select == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == None:\n",
    "                # print(2,word_select)\n",
    "                continue\n",
    "            else:\n",
    "                # print(3,word_select)\n",
    "                tmp_list.append(word_select.upper())\n",
    "    long_str = long_str.upper()\n",
    "    for word_select in word_list:\n",
    "        if (word_select.upper() in long_str) and (not word_select.upper() in tmp_list):\n",
    "            if word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0] != 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) != None:\n",
    "                # print(1, word_select)\n",
    "                long_str = long_str.replace(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0], '')\n",
    "                if re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == 'R':\n",
    "                    tmp_list.append(word_select.upper())\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                continue\n",
    "            elif word_select.upper() == 'JAVA':\n",
    "                continue\n",
    "            elif word_select.upper() == 'JAVASCRIPT':\n",
    "                long_str = long_str.replace('JAVASCRIPT', '')\n",
    "                tmp_list.append(word_select.upper())\n",
    "            elif word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == None:\n",
    "                # print(2, word_select)\n",
    "                continue\n",
    "            else:\n",
    "                # print(3, word_select)\n",
    "                tmp_list.append(word_select.upper())\n",
    "    long_str = long_str.replace('JAVASCRIPT', '')\n",
    "    for word_select in word_list:\n",
    "        if (word_select.upper() in long_str) and (not word_select.upper() in tmp_list):\n",
    "            if word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0] != 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) != None:\n",
    "                # print(1, word_select)\n",
    "                long_str = long_str.replace(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0], '')\n",
    "                if re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == 'R':\n",
    "                    tmp_list.append(word_select.upper())\n",
    "                # print(re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str)[0])\n",
    "                continue\n",
    "            elif word_select.upper() == 'JAVA':\n",
    "                tmp_list.append(word_select.upper())\n",
    "            elif word_select.upper() == 'R' and re.compile('[a-zA-Z]*R[a-zA-Z]*').search(long_str) == None:\n",
    "                # print(2, word_select)\n",
    "                continue\n",
    "            else:\n",
    "                # print(3, word_select)\n",
    "                tmp_list.append(word_select.upper())\n",
    "\n",
    "    # Replace Synonym\n",
    "    for word_key in synonym_dict:\n",
    "        for word_value in  synonym_dict[word_key]:\n",
    "            for num, operating_word in enumerate(tmp_list):\n",
    "                if operating_word.upper() == word_value.upper():\n",
    "                    tmp_list[num] = word_key\n",
    "    tmp_list = list(set(tmp_list))\n",
    "\n",
    "    tmp_str = ''\n",
    "    for n, w in enumerate(tmp_list):\n",
    "        tmp_str += w\n",
    "        if n < len(tmp_list) - 1:\n",
    "            tmp_str += ','\n",
    "\n",
    "    return tmp_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
